{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d75a69c",
   "metadata": {},
   "source": [
    "# Multi-class News Classification Notebook\n",
    "\n",
    "This notebook trains a multi-class text classification model on `news_balanced_categories.csv`.\n",
    "It uses the `headline` and `short_description` columns combined as input, and `category` as the label.\n",
    "\n",
    "**Features included**\n",
    "- Data loading & inspection\n",
    "- Basic text cleaning\n",
    "- Combining `headline` + `short_description`\n",
    "- Train / validation split\n",
    "- Tokenization and padding\n",
    "- Label encoding\n",
    "- Keras model (Embedding + Bidirectional LSTM)\n",
    "- Training, evaluation, and plotting metrics\n",
    "- Sample predictions\n",
    "\n",
    "Run this notebook cell-by-cell. Adjust hyperparameters (vocab_size, max_len, embedding_dim, epochs) for better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and load dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_path = \"/mnt/data/news_balanced_categories.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# inspect\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bc7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns, null counts and class distribution\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nNull counts:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text input (combine headline and short_description) and labels\n",
    "df['short_description'] = df['short_description'].fillna('')\n",
    "df['text'] = (df['headline'].astype(str) + \" . \" + df['short_description'].astype(str)).str.strip()\n",
    "\n",
    "# Drop rows with empty text or missing category\n",
    "df = df[df['text'].str.strip() != '']\n",
    "df = df.dropna(subset=['category'])\n",
    "\n",
    "print(\"Prepared dataset shape:\", df.shape)\n",
    "df[['text','category']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9586d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / validation split and label encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['category'].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "class_names = label_encoder.classes_.tolist()\n",
    "print(\"Classes ({}):\".format(len(class_names)), class_names)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_enc, test_size=0.2, stratify=y_enc, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Validation size:\", len(X_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and sequence padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 20000\n",
    "oov_token = \"<OOV>\"\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "max_len = 120\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "print(\"Vocabulary size (used):\", min(vocab_size, len(tokenizer.word_index)+1))\n",
    "print(\"Example sequence length:\", len(X_train_seq[0]), \"padded shape:\", X_train_pad.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Keras model (Embedding + Bidirectional LSTM)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_dim = 100\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a67a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 6\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77070142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (accuracy and loss)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and show classification report\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "y_pred_probs = model.predict(X_val_pad)\n",
    "y_pred = y_pred_probs.argmax(axis=1)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f881e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer and label encoder for later use\n",
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Saved tokenizer.pkl and label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8408547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction function\n",
    "def predict_text(texts, top_k=1):\n",
    "    seq = tokenizer.texts_to_sequences(texts)\n",
    "    pad = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    probs = model.predict(pad)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    labels = label_encoder.inverse_transform(preds)\n",
    "    return list(zip(texts, labels, probs.max(axis=1)))\n",
    "\n",
    "# Try on a few validation texts\n",
    "samples = list(X_val[:8])\n",
    "preds = predict_text(samples)\n",
    "for txt, lbl, conf in preds:\n",
    "    print(f\"Label: {lbl} (conf={conf:.3f})\\nText: {txt[:200]}\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba2c1c",
   "metadata": {},
   "source": [
    "1. If you have imbalanced classes, consider class weights or oversampling.\n",
    "2. Try experimenting with `vocab_size`, `max_len`, `embedding_dim`, and model architecture (CNN, transformers).\n",
    "3. For better performance, try transfer learning with pretrained embeddings (GloVe) or transformer models (BERT).\n",
    "4. If GPU is available, increase batch_size and epochs for more stable training.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
